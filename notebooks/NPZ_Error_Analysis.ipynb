{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NPZ Error Analysis\n",
        "\n",
        "This notebook helps you inspect NPZ segments and find where the model performs poorly.\n",
        "\n",
        "Workflow:\n",
        "1. Configure paths and labels.\n",
        "2. Build train/val splits and dataset stats.\n",
        "3. Load a checkpoint and run inference.\n",
        "4. Inspect misclassified and low-confidence samples.\n",
        "5. Visualize raw audio and sensor channels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from src.ReadSegments import ReadSegments, find_segment_paths, read_labels_from_segments\n",
        "from util.label_processor import LabelProcessor, DropLabel\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "except ImportError:\n",
        "    pd = None\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "DATA_ROOT = Path(\"data/MMDataset_segments_first5\")\n",
        "CKPT_PATH = Path(\"checkpoints/best_multimodal.pt\")\n",
        "\n",
        "TARGET_SR = 22050\n",
        "AUDIO_FMAX = 4000\n",
        "TRAIN_RATIO = 0.6\n",
        "SPLIT_SEED = 42\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "label_processor = LabelProcessor(\n",
        "    raw_to_norm={\n",
        "        \"no secretion\": \"no_secretion\",\n",
        "        \"no secretion sound\": \"no_secretion\",\n",
        "        \"no secretion sound (with hemf)\": \"no_secretion\",\n",
        "        \"3ml secretion\": \"secretion\",\n",
        "        \"3ml secretion m4\": \"secretion\",\n",
        "        \"5ml secretion m4\": \"secretion\",\n",
        "        \"5ml secretion\": \"secretion\",\n",
        "        \"3ml secretion (with hemf)\": \"secretion\",\n",
        "    },\n",
        "    fail_on_unknown=True,\n",
        ")\n",
        "\n",
        "def group_key_from_path(path: str) -> str:\n",
        "    base = os.path.basename(path)\n",
        "    if \"_win\" in base:\n",
        "        return base.split(\"_win\")[0]\n",
        "    return os.path.splitext(base)[0]\n",
        "\n",
        "def infer_group_label(group_paths, normalizer):\n",
        "    norm_label = None\n",
        "    for p in group_paths:\n",
        "        d = np.load(p, allow_pickle=True)\n",
        "        if \"label\" not in d:\n",
        "            raise KeyError(f\"Missing 'label' field in {p}\")\n",
        "        lab = d[\"label\"]\n",
        "        if isinstance(lab, np.ndarray) and lab.shape == ():\n",
        "            lab = lab.item()\n",
        "        current = normalizer(str(lab))\n",
        "        if norm_label is None:\n",
        "            norm_label = current\n",
        "        elif norm_label != current:\n",
        "            raise ValueError(f\"Mixed labels inside group {p}: '{norm_label}' vs '{current}'\")\n",
        "    if norm_label is None:\n",
        "        raise ValueError(\"Group contained no labels; cannot infer class\")\n",
        "    return norm_label\n",
        "\n",
        "def split_paths_by_group(paths, train_ratio=0.6, seed=42, label_normalizer=None):\n",
        "    groups = {}\n",
        "    for p in paths:\n",
        "        key = group_key_from_path(p)\n",
        "        groups.setdefault(key, []).append(p)\n",
        "\n",
        "    rng = random.Random(seed)\n",
        "\n",
        "    if label_normalizer is not None:\n",
        "        label_to_keys = {}\n",
        "        for k, ps in groups.items():\n",
        "            try:\n",
        "                norm_label = infer_group_label(ps, label_normalizer)\n",
        "            except DropLabel:\n",
        "                continue\n",
        "            label_to_keys.setdefault(norm_label, []).append(k)\n",
        "\n",
        "        train_keys = []\n",
        "        val_keys = []\n",
        "        for _, keys in sorted(label_to_keys.items()):\n",
        "            keys_sorted = sorted(keys)\n",
        "            rng.shuffle(keys_sorted)\n",
        "            n_train = int(train_ratio * len(keys_sorted))\n",
        "            if len(keys_sorted) > 1:\n",
        "                n_train = min(max(n_train, 1), len(keys_sorted) - 1)\n",
        "            train_keys.extend(keys_sorted[:n_train])\n",
        "            val_keys.extend(keys_sorted[n_train:])\n",
        "        rng.shuffle(train_keys)\n",
        "        rng.shuffle(val_keys)\n",
        "    else:\n",
        "        keys = sorted(groups.keys())\n",
        "        rng.shuffle(keys)\n",
        "        n_train = int(train_ratio * len(keys))\n",
        "        if len(keys) > 1:\n",
        "            n_train = min(max(n_train, 1), len(keys) - 1)\n",
        "        train_keys = keys[:n_train]\n",
        "        val_keys = keys[n_train:]\n",
        "\n",
        "    train_paths = [p for k in train_keys for p in sorted(groups[k])]\n",
        "    val_paths = [p for k in val_keys for p in sorted(groups[k])]\n",
        "    return train_paths, val_paths\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "all_paths = sorted(find_segment_paths(str(DATA_ROOT)))\n",
        "print(\"Total npz files:\", len(all_paths))\n",
        "if not all_paths:\n",
        "    raise RuntimeError(\"No npz files found. Update DATA_ROOT.\")\n",
        "\n",
        "raw_labels = read_labels_from_segments(all_paths)\n",
        "print(label_processor.summarize_counts(raw_labels))\n",
        "\n",
        "train_paths, val_paths = split_paths_by_group(\n",
        "    all_paths, train_ratio=TRAIN_RATIO, seed=SPLIT_SEED, label_normalizer=label_processor\n",
        ")\n",
        "print(\"Train segments:\", len(train_paths))\n",
        "print(\"Val segments:\", len(val_paths))\n",
        "print(\"Total groups:\", len({group_key_from_path(p) for p in all_paths}))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    audio_list = [b[\"audio\"] for b in batch]\n",
        "    audio_lengths = [a.shape[0] for a in audio_list]\n",
        "    max_T_audio = max(audio_lengths)\n",
        "    B = len(batch)\n",
        "\n",
        "    audio = torch.zeros(B, max_T_audio, dtype=torch.float32)\n",
        "    for i, (a, L) in enumerate(zip(audio_list, audio_lengths)):\n",
        "        audio[i, :L] = a\n",
        "\n",
        "    sensor_seqs = []\n",
        "    lengths = []\n",
        "    paths = []\n",
        "    raw_labels = []\n",
        "    norm_labels = []\n",
        "\n",
        "    for b in batch:\n",
        "        P = b[\"P\"]\n",
        "        Q = b[\"Q\"]\n",
        "\n",
        "        if P is None or Q is None:\n",
        "            T = b[\"audio\"].shape[0]\n",
        "            sensor = torch.zeros(T, 2, dtype=torch.float32)\n",
        "        else:\n",
        "            if P.dim() == 1:\n",
        "                P_2d = P.unsqueeze(-1)\n",
        "            else:\n",
        "                P_2d = P\n",
        "            if Q.dim() == 1:\n",
        "                Q_2d = Q.unsqueeze(-1)\n",
        "            else:\n",
        "                Q_2d = Q\n",
        "            P_main = P_2d[:, 0:1]\n",
        "            Q_main = Q_2d[:, 0:1]\n",
        "            sensor = torch.cat([P_main, Q_main], dim=-1)\n",
        "\n",
        "        sensor_seqs.append(sensor)\n",
        "        lengths.append(sensor.shape[0])\n",
        "        paths.append(b[\"path\"])\n",
        "        raw_labels.append(b.get(\"raw_label\"))\n",
        "        norm_labels.append(b.get(\"norm_label\"))\n",
        "\n",
        "    max_len = max(lengths)\n",
        "    sensor_padded = torch.zeros(B, max_len, 2, dtype=torch.float32)\n",
        "    for i, (seq, L) in enumerate(zip(sensor_seqs, lengths)):\n",
        "        sensor_padded[i, :L, :] = seq\n",
        "\n",
        "    lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
        "    audio_lengths_tensor = torch.tensor(audio_lengths, dtype=torch.long)\n",
        "\n",
        "    labels = torch.stack([b[\"label_id\"] for b in batch], dim=0)\n",
        "\n",
        "    return {\n",
        "        \"audio\": audio,\n",
        "        \"audio_lengths\": audio_lengths_tensor,\n",
        "        \"sensor\": sensor_padded,\n",
        "        \"sensor_lengths\": lengths_tensor,\n",
        "        \"label\": labels,\n",
        "        \"paths\": paths,\n",
        "        \"raw_label\": raw_labels,\n",
        "        \"norm_label\": norm_labels,\n",
        "    }\n",
        "\n",
        "train_ds = ReadSegments(\n",
        "    train_paths,\n",
        "    target_sample_rate=TARGET_SR,\n",
        "    label_normalizer=label_processor,\n",
        "    group_normalize=True,\n",
        ")\n",
        "val_ds = ReadSegments(\n",
        "    val_paths,\n",
        "    target_sample_rate=TARGET_SR,\n",
        "    label_normalizer=label_processor,\n",
        "    label2id=train_ds.label2id,\n",
        "    label_map_paths=train_paths,\n",
        "    group_normalize=True,\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "print(\"Label map:\", train_ds.label2id)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def _replace_bn_with_gn(module, num_groups=8):\n",
        "    for name, child in module.named_children():\n",
        "        if isinstance(child, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
        "            num_channels = child.num_features\n",
        "            gn = nn.GroupNorm(\n",
        "                num_groups=min(num_groups, num_channels),\n",
        "                num_channels=num_channels,\n",
        "                affine=True,\n",
        "            )\n",
        "            setattr(module, name, gn)\n",
        "        else:\n",
        "            _replace_bn_with_gn(child, num_groups=num_groups)\n",
        "\n",
        "class AudioResNetEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sample_rate=48000,\n",
        "        n_mels=64,\n",
        "        n_fft=1024,\n",
        "        hop_length=512,\n",
        "        out_dim=128,\n",
        "        f_min=0.0,\n",
        "        f_max=None,\n",
        "        use_pretrained=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        max_freq = sample_rate / 2 if f_max is None else f_max\n",
        "        self.melspec = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=sample_rate,\n",
        "            n_fft=n_fft,\n",
        "            hop_length=hop_length,\n",
        "            n_mels=n_mels,\n",
        "            f_min=f_min,\n",
        "            f_max=max_freq,\n",
        "        )\n",
        "        self.db = torchaudio.transforms.AmplitudeToDB()\n",
        "\n",
        "        weights = models.ResNet18_Weights.IMAGENET1K_V1 if use_pretrained else None\n",
        "        self.backbone = models.resnet18(weights=weights)\n",
        "        _replace_bn_with_gn(self.backbone, num_groups=8)\n",
        "        self.backbone.conv1 = nn.Conv2d(\n",
        "            1, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Linear(in_features, out_dim)\n",
        "\n",
        "    def forward(self, audio):\n",
        "        x = audio.unsqueeze(1)\n",
        "        x = x.squeeze(1)\n",
        "        mel = self.melspec(x)\n",
        "        mel_db = self.db(mel)\n",
        "        mel_db = mel_db.unsqueeze(1)\n",
        "        feat = self.backbone(mel_db)\n",
        "        return feat\n",
        "\n",
        "class InceptionBlock1D(nn.Module):\n",
        "    def __init__(self, in_channels, n_filters, kernel_sizes, bottleneck_channels):\n",
        "        super().__init__()\n",
        "        use_bottleneck = bottleneck_channels > 0 and in_channels > 1\n",
        "        self.bottleneck = (\n",
        "            nn.Conv1d(in_channels, bottleneck_channels, kernel_size=1, bias=False)\n",
        "            if use_bottleneck\n",
        "            else None\n",
        "        )\n",
        "        conv_in = bottleneck_channels if use_bottleneck else in_channels\n",
        "        self.convs = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv1d(\n",
        "                    conv_in,\n",
        "                    n_filters,\n",
        "                    kernel_size=k,\n",
        "                    padding=k // 2,\n",
        "                    bias=False,\n",
        "                )\n",
        "                for k in kernel_sizes\n",
        "            ]\n",
        "        )\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=1, padding=1)\n",
        "        self.pool_conv = nn.Conv1d(in_channels, n_filters, kernel_size=1, bias=False)\n",
        "        out_channels = n_filters * (len(kernel_sizes) + 1)\n",
        "        self.bn = nn.GroupNorm(\n",
        "            num_groups=min(8, out_channels),\n",
        "            num_channels=out_channels,\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.bottleneck is not None:\n",
        "            x = self.bottleneck(x)\n",
        "        outs = [conv(x) for conv in self.convs]\n",
        "        outs.append(self.pool_conv(self.maxpool(x_in)))\n",
        "        x = torch.cat(outs, dim=1)\n",
        "        x = self.bn(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "class InceptionTimeEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim=2,\n",
        "        out_dim=128,\n",
        "        n_filters=32,\n",
        "        kernel_sizes=(9, 19, 39),\n",
        "        bottleneck_channels=32,\n",
        "        n_blocks=6,\n",
        "        use_residual=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.use_residual = use_residual\n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.residuals = nn.ModuleDict()\n",
        "\n",
        "        in_channels = input_dim\n",
        "        res_in_channels = input_dim\n",
        "        for i in range(n_blocks):\n",
        "            block = InceptionBlock1D(\n",
        "                in_channels=in_channels,\n",
        "                n_filters=n_filters,\n",
        "                kernel_sizes=kernel_sizes,\n",
        "                bottleneck_channels=bottleneck_channels,\n",
        "            )\n",
        "            self.blocks.append(block)\n",
        "            out_channels = n_filters * (len(kernel_sizes) + 1)\n",
        "            if self.use_residual and (i + 1) % 3 == 0:\n",
        "                self.residuals[str(i)] = nn.Sequential(\n",
        "                    nn.Conv1d(res_in_channels, out_channels, kernel_size=1, bias=False),\n",
        "                    nn.GroupNorm(num_groups=min(8, out_channels), num_channels=out_channels),\n",
        "                )\n",
        "                res_in_channels = out_channels\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.proj = nn.Linear(in_channels, out_dim)\n",
        "        self.res_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        _ = lengths\n",
        "        x = x.transpose(1, 2)\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            if self.use_residual and i % 3 == 0:\n",
        "                res_input = x\n",
        "            x = block(x)\n",
        "            if self.use_residual and (i + 1) % 3 == 0:\n",
        "                res = self.residuals[str(i)](res_input)\n",
        "                x = self.res_relu(x + res)\n",
        "        feats = self.global_pool(x).squeeze(-1)\n",
        "        return self.proj(feats)\n",
        "\n",
        "class MultiModalLateFusionNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        audio_feat_dim=128,\n",
        "        sensor_feat_dim=128,\n",
        "        num_classes=2,\n",
        "        sample_rate=48000,\n",
        "        f_min=0.0,\n",
        "        f_max=None,\n",
        "        use_pretrained=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.audio_encoder = AudioResNetEncoder(\n",
        "            sample_rate=sample_rate,\n",
        "            out_dim=audio_feat_dim,\n",
        "            f_min=f_min,\n",
        "            f_max=f_max,\n",
        "            use_pretrained=use_pretrained,\n",
        "        )\n",
        "        self.sensor_encoder = InceptionTimeEncoder(\n",
        "            input_dim=2,\n",
        "            out_dim=sensor_feat_dim,\n",
        "        )\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(audio_feat_dim + sensor_feat_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, audio, sensor, sensor_lengths, drop_audio=False, drop_sensor=False):\n",
        "        fa = self.audio_encoder(audio)\n",
        "        fs = self.sensor_encoder(sensor, sensor_lengths)\n",
        "\n",
        "        if drop_audio:\n",
        "            fa = torch.zeros_like(fa)\n",
        "        if drop_sensor:\n",
        "            fs = torch.zeros_like(fs)\n",
        "\n",
        "        fused = torch.cat([fa, fs], dim=-1)\n",
        "        logits = self.fusion(fused)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model = None\n",
        "id2label = None\n",
        "\n",
        "if not CKPT_PATH.exists():\n",
        "    print(\"Checkpoint not found:\", CKPT_PATH)\n",
        "else:\n",
        "    ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
        "    label2id = ckpt.get(\"label2id\", train_ds.label2id)\n",
        "    id2label = {v: k for k, v in label2id.items()}\n",
        "    model = MultiModalLateFusionNet(\n",
        "        audio_feat_dim=128,\n",
        "        sensor_feat_dim=128,\n",
        "        num_classes=len(label2id),\n",
        "        sample_rate=TARGET_SR,\n",
        "        f_max=AUDIO_FMAX,\n",
        "        use_pretrained=False,\n",
        "    )\n",
        "    try:\n",
        "        model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
        "    except RuntimeError as exc:\n",
        "        print(\"Checkpoint load failed:\", exc)\n",
        "        model = None\n",
        "    if model is not None:\n",
        "        model.to(DEVICE)\n",
        "        model.eval()\n",
        "        print(\"Loaded checkpoint with labels:\", id2label)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def run_inference(model, loader, device, id2label):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    rows = []\n",
        "    for batch in loader:\n",
        "        audio = batch[\"audio\"].to(device)\n",
        "        sensor = batch[\"sensor\"].to(device)\n",
        "        lengths = batch[\"sensor_lengths\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        paths = batch[\"paths\"]\n",
        "        raw_labels = batch.get(\"raw_label\", [None] * len(paths))\n",
        "        norm_labels = batch.get(\"norm_label\", [None] * len(paths))\n",
        "\n",
        "        logits = model(audio, sensor, lengths, drop_audio=False, drop_sensor=False)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        loss = ce(logits, labels)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        for i in range(len(paths)):\n",
        "            true_id = int(labels[i].item())\n",
        "            pred_id = int(preds[i].item())\n",
        "            prob_true = float(probs[i, true_id].item())\n",
        "            prob_pred = float(probs[i, pred_id].item())\n",
        "            rows.append({\n",
        "                \"path\": paths[i],\n",
        "                \"group\": group_key_from_path(paths[i]),\n",
        "                \"true_id\": true_id,\n",
        "                \"pred_id\": pred_id,\n",
        "                \"true_label\": id2label.get(true_id, str(true_id)),\n",
        "                \"pred_label\": id2label.get(pred_id, str(pred_id)),\n",
        "                \"prob_true\": prob_true,\n",
        "                \"prob_pred\": prob_pred,\n",
        "                \"loss\": float(loss[i].item()),\n",
        "                \"correct\": int(true_id == pred_id),\n",
        "                \"raw_label\": raw_labels[i],\n",
        "                \"norm_label\": norm_labels[i],\n",
        "            })\n",
        "\n",
        "    if pd is not None:\n",
        "        return pd.DataFrame(rows)\n",
        "    return rows\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "EVAL_SPLIT = \"val\"  # 'train' or 'val'\n",
        "eval_loader = val_loader if EVAL_SPLIT == \"val\" else train_loader\n",
        "\n",
        "if model is None:\n",
        "    print(\"Load a checkpoint before running inference.\")\n",
        "else:\n",
        "    df = run_inference(model, eval_loader, DEVICE, id2label)\n",
        "    if pd is not None:\n",
        "        display(df.head())\n",
        "    else:\n",
        "        print(df[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def compute_confusion(labels, preds, num_classes):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(labels, preds):\n",
        "        cm[int(t), int(p)] += 1\n",
        "    return cm\n",
        "\n",
        "def compute_precision_recall(cm):\n",
        "    precision = np.diag(cm) / np.maximum(cm.sum(axis=0), 1)\n",
        "    recall = np.diag(cm) / np.maximum(cm.sum(axis=1), 1)\n",
        "    return precision, recall\n",
        "\n",
        "if model is not None:\n",
        "    if pd is not None:\n",
        "        labels = df[\"true_id\"].to_numpy()\n",
        "        preds = df[\"pred_id\"].to_numpy()\n",
        "    else:\n",
        "        labels = [row[\"true_id\"] for row in df]\n",
        "        preds = [row[\"pred_id\"] for row in df]\n",
        "\n",
        "    cm = compute_confusion(labels, preds, num_classes=len(id2label))\n",
        "    precision, recall = compute_precision_recall(cm)\n",
        "\n",
        "    print(\"Accuracy:\", float(np.mean(np.array(labels) == np.array(preds))))\n",
        "    if pd is not None:\n",
        "        cm_df = pd.DataFrame(cm,\n",
        "                            index=[id2label[i] for i in range(len(id2label))],\n",
        "                            columns=[id2label[i] for i in range(len(id2label))])\n",
        "        display(cm_df)\n",
        "        pr_df = pd.DataFrame({\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "        }, index=[id2label[i] for i in range(len(id2label))])\n",
        "        display(pr_df)\n",
        "    else:\n",
        "        print(\"Confusion matrix:\n\", cm)\n",
        "        print(\"Precision:\", precision)\n",
        "        print(\"Recall:\", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if model is not None:\n",
        "    if pd is not None:\n",
        "        print(\"Misclassified samples (top 20 by loss):\")\n",
        "        display(df[df[\"correct\"] == 0].sort_values(\"loss\", ascending=False).head(20))\n",
        "        print(\"Hard correct samples (low prob_true):\")\n",
        "        display(df[df[\"correct\"] == 1].sort_values(\"prob_true\", ascending=True).head(20))\n",
        "    else:\n",
        "        mis = [row for row in df if row[\"correct\"] == 0]\n",
        "        mis = sorted(mis, key=lambda r: r[\"loss\"], reverse=True)[:20]\n",
        "        print(mis)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if model is not None and pd is not None:\n",
        "    group_summary = (\n",
        "        df.groupby(\"group\")\n",
        "          .agg(total=(\"path\", \"count\"),\n",
        "               correct=(\"correct\", \"sum\"),\n",
        "               acc=(\"correct\", \"mean\"),\n",
        "               mean_loss=(\"loss\", \"mean\"))\n",
        "          .sort_values(\"acc\", ascending=True)\n",
        "    )\n",
        "    display(group_summary.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def plot_npz(path, pred_row=None, target_sr=TARGET_SR):\n",
        "    d = np.load(path, allow_pickle=True)\n",
        "    audio = d[\"audio\"].astype(np.float32)\n",
        "    sr = int(d.get(\"audio_rate_hz\", 48000))\n",
        "    label = d.get(\"label\")\n",
        "    if isinstance(label, np.ndarray) and label.shape == ():\n",
        "        label = label.item()\n",
        "\n",
        "    audio_t = torch.from_numpy(audio)\n",
        "    if sr != target_sr:\n",
        "        audio_t = torchaudio.functional.resample(audio_t, sr, target_sr)\n",
        "        sr = target_sr\n",
        "\n",
        "    mel = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=sr,\n",
        "        n_mels=64,\n",
        "        n_fft=1024,\n",
        "        hop_length=512,\n",
        "        f_max=AUDIO_FMAX,\n",
        "    )(audio_t.unsqueeze(0))\n",
        "    mel_db = torchaudio.transforms.AmplitudeToDB()(mel).squeeze(0).numpy()\n",
        "\n",
        "    sensor_all = d[\"sensor_values\"].astype(np.float32)\n",
        "    sensor_cols = [str(c) for c in d[\"sensor_cols\"]]\n",
        "    p_idx = [i for i, name in enumerate(sensor_cols) if \"P_\" in name]\n",
        "    q_idx = [i for i, name in enumerate(sensor_cols) if \"F_\" in name]\n",
        "\n",
        "    P = sensor_all[:, p_idx[0]] if p_idx else None\n",
        "    Q = sensor_all[:, q_idx[0]] if q_idx else None\n",
        "\n",
        "    title = f\"label={label}\"\n",
        "    if pred_row is not None:\n",
        "        title += f\" | pred={pred_row.get('pred_label')} | prob_true={pred_row.get('prob_true'):.3f}\"\n",
        "\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
        "    axes[0].plot(audio_t.numpy())\n",
        "    axes[0].set_title(\"Audio waveform\")\n",
        "    axes[1].imshow(mel_db, aspect=\"auto\", origin=\"lower\")\n",
        "    axes[1].set_title(\"Mel spectrogram (dB)\")\n",
        "    if P is not None or Q is not None:\n",
        "        if P is not None:\n",
        "            axes[2].plot(P, label=\"P\")\n",
        "        if Q is not None:\n",
        "            axes[2].plot(Q, label=\"Q\")\n",
        "        axes[2].legend()\n",
        "    axes[2].set_title(\"Sensor channels\")\n",
        "    fig.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if model is not None and pd is not None and len(df):\n",
        "    sample = df.sort_values(\"loss\", ascending=False).iloc[0]\n",
        "    plot_npz(sample[\"path\"], sample)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}